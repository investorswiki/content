---
keywords: Investing,Fundamental Analysis,Tools for Fundamental Analysis,Tools
title: Conditional Probability
description: Conditional likelihood is the probability of an event or outcome happening in light of the occurrence of another previous event or outcome.
---

# Conditional Probability
## What Is Conditional Probability?

Conditional likelihood is defined as the probability of an event or outcome happening, in light of the occurrence of a previous event or outcome. Conditional likelihood is calculated by increasing the [probability](/compound-likelihood) of the former event by the refreshed likelihood of the succeeding, or conditional, event.

For instance:

- Event An is that an individual applying for college will be accepted. There is a 80% chance that this individual will be accepted to college.
- Event B is that this individual will given dorm house. Residence housing may be given to 60% of the accepted students in general.
- P (Accepted and residence housing) = P (Dormitory Housing | Accepted) P (Accepted) = (0.60)*(0.80) = 0.48.

A conditional likelihood would take a gander at these two events in relationship with each other, for example, the likelihood that you are both accepted to college, **and** you are furnished with residence housing.

Conditional likelihood can be stood out from [unconditional probability](/unconditional_probability). Unconditional likelihood alludes to the probability that an event will happen independent of whether some other events have occurred or some other conditions are available.

## Grasping Conditional Probability

As previously stated, conditional probabilities are contingent on a [previous result](/prior_probability). It likewise makes a number of suspicions. For instance, assume you are drawing three marbles — red, blue, and green — from a bag. Each marble has an equivalent chance of being drawn. What is the conditional likelihood of drawing the red marble after previously drawing the blue one?

To start with, the likelihood of drawing a blue marble is around 33% on the grounds that it is one potential outcome out of three. Expecting this first event happens, there will two marbles stay, with each having a half chance of being drawn. So the chance of drawing a blue marble after previously drawing a red marble would be around 16.5% (33% x half).

> Conditional likelihood is utilized in various fields, for example, [insurance](/plausible greatest misfortune pml), politics, and a wide range of fields of math.
>

As one more guide to give further knowledge into this concept, look at that as a fair pass on has been rolled and you are approached to give the likelihood that it was a five. There are six similarly probable outcomes, so your response is 1/6.

In any case, envision if before you reply, you get extra data that the number rolled was odd. Since there are just three odd numbers that are conceivable, one of which is five, you would positively change your estimate for the probability that a five was moved from 1/6 to 1/3.

This **revised** likelihood that an event **A** has occurred, taking into account the extra data that one more event **B** has most certainly occurred on this trial of the investigation, is called the **conditional likelihood of** **A** **given** **B** and is signified by P(A|B).

## Conditional Probability Formula
>
> **P(B|A) = P(A and B)/P(A)**
>

**Or:**

>
> **P(B|A) = P(A∩B)/P(A)**
>
>
> **Where**
>
>
> **P = Probability**
>
>
> **A = Event A**
>
>
> **B = Event B**
>
## One more Example of Conditional Probability

As another model, assume a student is applying for admission to a university and desires to receive a scholastic grant. The school to which they are applying acknowledges 100 of each and every 1,000 candidates (10%) and awards scholastic scholarships to 10 of each and every 500 students who are accepted (2%).

Of the grant beneficiaries, half of them likewise receive university stipends for books, dinners, and housing. For the students, the chance of them being accepted then getting a grant is .2% (.1 x .02). The chance of them being accepted, getting the grant, then likewise getting a stipend for books, and so on is .1% (.1 x .02 x .5).

## Conditional Probability versus Joint Probability and Marginal Probability

**Conditional probability**: p(A|B) is the likelihood of event A happening, given that event B happens. For instance, given that you drew a red card, what's the likelihood that it's a four (p(four|red))=2/26=1/13. So out of the 26 red cards (given a red card), there are two fours so 2/26=1/13.

**Marginal probability**: the likelihood of an event happening (p(A)), it could be considered an unconditional likelihood. It isn't adapted on another event. Model: the likelihood that a card drawn is red (p(red) = 0.5). Another model: the likelihood that a card drawn is a 4 (p(four)=1/13).

[Joint probability](/jointprobability): p(A and B). The likelihood of event A **and** event B happening. It is the likelihood of the convergence of at least two events. The likelihood of the convergence of An and B might be written p(A ∩ B). Model: the likelihood that a card is a four and red =p(four and red) = 2/52=1/26. (There are two red fours in a deck of 52, the 4 of hearts and the 4 of diamonds).

## Bayes' Theorem

[Bayes' theorem](/bayes-theorem), named after eighteenth century British mathematician Thomas Bayes, is a mathematical formula for deciding conditional likelihood. The theorem gives a method for reexamining existing predictions or speculations (update probabilities) given new or extra evidence. In finance, Bayes' theorem can be utilized to rate the [risk](/risk) of lending money to likely borrowers.

> Bayes' theorem is appropriate to and widely utilized in machine learning.
>

Bayes' theorem is additionally called Bayes' Rule or Bayes' Law and is the foundation of the field of Bayesian statistics. This set of rules of likelihood permits one to refresh their predictions of events happening in light of new data that has been received, making for better and more dynamic estimates.

## The Bottom Line

Conditional likelihood inspects the probability of an event happening in light of the probability of a previous event happening. The subsequent event is dependent on the main event. It is calculated by duplicating the likelihood of the principal event by the likelihood of the subsequent event.

## Features
- Bayes' theorem is a mathematical formula utilized in computing conditional likelihood.
- It is many times stated as the likelihood of B given An and is written as P(B|A), where the likelihood of B relies upon that of An incident.
- Probabilities are classified as either conditional, marginal, or joint.
- Conditional likelihood alludes to the chances that some outcome happens given that another event has likewise occurred.
- Conditional likelihood can be diverged from unconditional likelihood.
## FAQ
### What Is Compound Probability?
Compound likelihood hopes to decide the probability of two independent events happening. Compound likelihood increases the likelihood of the principal event by the likelihood of the subsequent event. The most common model is that of a coin flipped two times and the determination in the event that the subsequent outcome will be something very similar or not the same as the first.
### How Do You Calculate Conditional Probability?
Conditional likelihood is calculated by increasing the likelihood of the former event by the likelihood of the succeeding or conditional event. Conditional likelihood takes a gander at the likelihood of one event happening in light of the likelihood of a former event occurring.
### What Is a Conditional Probability Calculator?
A conditional likelihood calculator is an online instrument that will compute conditional likelihood. It will give the likelihood of the principal event and the subsequent event happening. A conditional likelihood calculator saves the client from doing the science physically.
### What Is Prior Probability?
Prior likelihood is the likelihood of an event happening before any data has been gathered to decide the likelihood. It is the not entirely set in stone by a prior conviction. Prior likelihood is a part of Bayesian statistical derivation.
### What Is the Difference Between Probability and Conditional Probability?
Likelihood takes a gander at the probability of one event happening. Conditional likelihood takes a gander at two events happening corresponding to each other. It takes a gander at the likelihood of a subsequent event happening in light of the likelihood of the principal event happening.
